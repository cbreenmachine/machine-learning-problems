---
title: "STOR 565 Spring 2019 Homework 5"
author: "Coleman Breen"
output:
  pdf_document: default
  html_document: default
subtitle: \textbf{Due on 03/21/2018 in Class}
header-includes: \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(ISLR)) { install.packages("ISLR", repos = "http://cran.us.r-project.org"); library("ISLR") }
if(!require(class)) { install.packages("class", repos = "http://cran.us.r-project.org"); library("class") }
if(!require(e1071)) { install.packages("e1071", repos = "http://cran.us.r-project.org"); library("e1071") }
if(!require(splines)) { install.packages("splines", repos = "http://cran.us.r-project.org"); library("splines") }
```
\theoremstyle{definition}
\newtheorem*{hint}{Hint}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

*Remark.* Credits for **Theoretical Part** and **Computational Part** are in total *100 pt* (40 pt for theoretical and 60pt for computational) please complete your computational report below in the **RMarkdown** file and submit your printed PDF homework created by it.

## Computational Part

 **You are supposed to finish the Computational Part in a readable report manner for each dataset-based analysis problem. Other than what you are asked to do, you can decide any details on your own discretion.** Also goto R demonstrations in the Lecture 6 folder on Sakai. There you will find a working example for LDA, QDA and k-nn for the titanic data as well as a much more extensive demonstartion on k-nn in the folder under classification-knn in the same folder in Sakai. 
 
 You may need some of these packages:
 
```{r}
library(MASS)
library(caret) 
library(class)
```
 


In particular, the MASS package for doing LDA, QDA and the class package for doing K-nn. 

1. The following data set is coming from a Kaggle competition that came out on November 12, 2015. Here is the description from the competition:

*Time magazine noted Mashable as one of the 25 best blogs in 2009, and described it as a "one stop shop" for social media. As of November 2015, [Mashable] had over 6,000,000 Twitter followers and over 3,200,000 fans on Facebook. In this problem, you'll use data from thousands of articles published by Mashable in a period of two years to see which variables predict the popularity of an article*.

**Load and read more about the data**

- Load the data *OnlineNewsPopularityTraining.csv*, which contains a large portion of the data set from the above competition.

- Read the variable descriptions for the variables at this website: [UCI website](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)

- A binary label has been added to the data set `popular`, which specifies whether or not each website is considered a popular website (0 for popular and 1 for not popular). 

- `popular` was created by assigning 1 to rows with `shares` values greater than 3300, and zero otherwise.


**Prepare the data**

- Remove the variables *shares*, *url* and  *timedelta* from the dataset.

```{r clean_website, warning=FALSE, message=FALSE}
#--> Load
library(tidyverse)
train.raw <- read.csv("OnlineNewsPopularityTraining.csv")
train <- select(train.raw, -c(shares, url, timedelta))

test.raw <- read.csv("OnlineNewsPopularityTest.csv")
test <- select(test.raw, -c(shares, url, timedelta))

#--> Display
head(train)
```

We'll start by removing predictors that don't make sense because they violate the continuous and normality assumptions made by LDA. I am making these decisions based on reading the descriptions of the data on UCI's website. The first group of variables to remove is the `weekday_is_x` because they are binary. The second group to remove is `data_channel_is_x`, again because they are binary.

```{r removeDiscrete}
removeDiscrete <- function(input){
  output <- input %>%
    # Days of the week (binary)
    select(-c(weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, 
             weekday_is_thursday, weekday_is_friday, weekday_is_saturday,
             weekday_is_sunday, is_weekend)) %>%
    # Type of article (binary)
    select(-c(data_channel_is_entertainment, data_channel_is_bus,
            data_channel_is_socmed, data_channel_is_tech,
            data_channel_is_world, data_channel_is_lifestyle)) %>%
    # Integer values (not continuous)
    select(-c(n_tokens_title, n_tokens_content, 
              n_non_stop_words, n_non_stop_unique_tokens,
              num_hrefs, num_self_hrefs, num_imgs, num_videos,
              num_keywords, kw_min_min, kw_max_min, kw_min_max,
              kw_max_max, kw_avg_min,
              LDA_00, LDA_01, LDA_02, LDA_03, LDA_04,
              self_reference_min_shares, self_reference_max_shares,
              self_reference_avg_sharess)) %>%
    # Don't look normal enough
    select(-c(kw_max_avg, title_sentiment_polarity,
              abs_title_subjectivity, title_subjectivity,
              min_negative_polarity, max_negative_polarity,
              min_positive_polarity, max_positive_polarity,
              abs_title_sentiment_polarity))
  return(output)
}
```

We'll perform the first round of culling on the train and test sets and then look at which other variables to remove and correct using plots.

```{r exploratory01}
#--> First round of cropping
train2 <- removeDiscrete(train)
test2 <- removeDiscrete(test)

#--> Plot
train2[1:12] %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

It appears that there are some outliers that are disrupting the normality of some of the metrics. We'll use a function to fix outliers below.

```{r removeOutliers}
removeOutliers <- function(input){
  output <- input %>%
    filter(n_unique_tokens < max(input$n_unique_tokens)) %>%
    filter(n_unique_tokens != 0) %>%
    filter(kw_min_avg != 0)  
  return(output)
}

```

```{r exploratory02}
#--> Second round of cleaning
train3 <- removeOutliers(train2)

#--> Plot
train3 %>%
  filter(popular==1) %>%
  select(-popular) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram() +
  ggtitle("Popular")

train3 %>%
  filter(popular==0) %>%
  select(-popular) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram() +
  ggtitle("Unpopular")
```

It looks like (besides `popular`) all of our remaining variables are roughly normally distributed conditioned on class. We could perform a transform on `kw_avg_avg` but for the sake of this analysis, our predictors are normal enough.

```{r assign_names}
train.clean <- train3
test.clean <- test2
```

```{r confusion}
#--> Confusion matrix function
confusion <- function(yhat, y, quietly = FALSE){
  
  if(!quietly)
    message("yhat is the vector of predicted outcomes, possibly a factor.\n
          Sensitivity = (first level predicted) / (first level actual) \n
          Specificity = (second level predicted) / (second level actual)")
  
  if(!is.factor(y) & is.factor(yhat))
    y <- as.factor(y)
  
  if(!all.equal(levels(yhat), levels(y)))
    stop("Factor levels of yhat and y do not match.")
  
  confusion_mat <- table(yhat, y, deparse.level = 2)
  stats <- data.frame(sensitivity = confusion_mat[1, 1]/sum(confusion_mat[, 1]),
                                 specificity = confusion_mat[2, 2]/sum(confusion_mat[, 2]))
  
  return(list(confusion_mat = confusion_mat, stats = stats))
}
```

**Questions**

(a) (15 points) The aim of this computational exercise is to prepare a classifier to predict whether or not a new website will be popular, i.e. classification by the `popular` variable in the dataset. You will do so using

- LDA
- QDA
- K-nearest neighbors

For each of the methods, 

1) carefully describe how you choose any thresholds or tuning parameters. 

2) list the predictors you would remove, if any, before fitting your models.

**You must justify your answers by specifically naming concepts studied in this course.** You also might want to justify your choices with summaries or plots of the data. Please do not print large amounts of data in the output.

I am being intentionally vague here because I want to see how you would handle such a data set in practice. All I ask is that you give proper justification for whatever you are doing. For example: the data contains indicator variables for different days of the week (weekday_is_monday etc). When doing LDA **I would remove these sorts of variables** as LDA inherently assumes that the features are continuous (and have a normal distribution).

(b) (30 points)For **each of the methods** listed in (a):

1) Fit a model to predict `popular` class labels, consistent with your answer in (a). 

2) Briefly discuss your results.

**You must show summary output of this model, along with plots and other documentation.**

*LDA*

```{r fit_lda}
#--> Fit model
mlda <- lda(popular ~ ., data = train.clean)
mlda

#--> Predict
lda.pred <- predict(mlda, newdata = test.clean)

#--> Confusion matrix
confusion(lda.pred$class, test.clean$popular)
```

1) See above for model. 
2) We used the following variables in our LDA model:
- `n_unique_tokens`
- `average_token_length`
- `kw_avg_max`
- `kw_min_avg`
- `kw_avg_avg`
- `global_subjectivity`
- `global_sentiment_polarity`
- `global_rate_positive_words`
- `global_rate_negative_words`
- `rate_positive_words`
- `rate_negative_words`
- `avg_positive_polarity`  
- `avg_negative_polarity`

These variables are the one that are continuous and normal enough to work as features for LDA. I outlined the pipeline I used to pick these variables above. Additionally, they are not collinear to the point of setting off any issues in the LDA function so they meet all of the necessary criteria.

The linear discriminants are printed above. Group means are not terribly different between `popular` and `unpopular` articles. The exceptions are `kw_min_avg` and `kw_avg_avg`. Since these predictors have to do with the number of shares based on certain keywords, it makes sense that they'd be good at distinguishing between an article that was shared many times and one that was not. 

*QDA*

There is some collinearity that does not throw off LDA but is a problem for QDA. We'll correct that here.

```{r qda_prep}
x <- dplyr::select(train.clean, -popular)
abs(cov(as.matrix(x))) 
```

```{r fit_qda}
#--> Fit model
mqda <- qda(popular ~ n_unique_tokens+average_token_length+
              global_subjectivity+global_sentiment_polarity+
              global_rate_positive_words+global_rate_negative_words+
              avg_positive_polarity+avg_negative_polarity,
            data = train.clean)
mqda

qda.pred <- predict(mqda, newdata = test.clean)

# Many actual survivors predicted to die 
confusion(qda.pred$class, test.clean$popular, quietly = TRUE)
```
1) See above for model.

2) I included the following variables in QDA (note that I had to eleminate the `kw_x` and `rate_positive/negative_words` variables to reduce covariance; see above for explanation on why I eleminated certain variables):
- `n_unique_tokens`
- `average_token_length`
- `global_subjectivity`
- `global_sentiment_polarity`
- `global_rate_positive_words`
- `global_rate_negative_words`
- `avg_positive_polarity`
- `avg_negative_polarity`

Without the aforementioned variables, there are not huge differences in the group means. However, our classifier preformed reasonably well. I will wait until the end to discuss performance.

*KNN*

I will start with using cross-validation to pick the best k number of neighbors to use. This pipeline relies heavily on work done by [Vijayakumar Jawaharlal](https://rstudio-pubs-static.s3.amazonaws.com/16444_caf85a306d564eb490eebdbaf0072df2.html).

```{r cross_validate_knn}
#--> Conver to factor
train.clean$popular <- as.factor(train.clean$popular)
set.seed(919)

#--> 10-fold cross validation
ctrl <- trainControl(method="repeatedcv",repeats = 3) 
knnFit <- train(popular ~ ., data = train.clean, method = "knn", 
                trControl = ctrl, preProcess = c("center","scale"), 
                tuneLength = 20)

#--> Output of kNN fit
knnFit

#--> Plot 
plot(knnFit)
```

```{r fit_knn}
ktrain <- dplyr::select(train.clean, -popular)
ktest <- dplyr::select(test.clean, -popular)

mknn <- knn(ktrain, ktest, cl = train.clean$popular, k = 43)
confusion(mknn, test.clean$popular)
```

1) See above for model. 
2) We used the following variables in our model (the same ones from LDA; see above for explanation on why I eleminated certain variables):
- `n_unique_tokens`
- `average_token_length`
- `kw_avg_max`
- `kw_min_avg`
- `kw_avg_avg`
- `global_subjectivity`
- `global_sentiment_polarity`
- `global_rate_positive_words`
- `global_rate_negative_words`
- `rate_positive_words`
- `rate_negative_words`
- `avg_positive_polarity`  
- `avg_negative_polarity`  

Using 10-fold cross validation on our training data, I found there to be an (almost) monotonic increase in the model accuracy as we increase the number of k neighbors. For tractability, I picked 43, which is the max number that our cross validation tested. If this were a model for a company, I would increase k until we found a global maximum in accuracy. However, to let this file run in a reasonable amount of time, I am selecting k = 43. I will discuss model accuracy below.

(c) (15 points) Download the test data *OnlineNewsPopularityTest.csv*. Predict `popular` class labels using each of the models in (b). Then:

1) Discuss the performance of each method using assessment measures such as MSPE, sensitivity, and specificity (see slide 68-69 for definitions of these objects; here popularity (class label 1) counts as "positives" and not popularity (class label 0) counts as negatives).  

I predicted in the same cells that I built the model. I will show a summary of the sensisitivity and specificity of each of our three models below.

```{r compare_models}
print("LDA")
confusion(lda.pred$class, test.clean$popular, quietly = TRUE)

print("QDA")
confusion(qda.pred$class, test.clean$popular, quietly = TRUE)

print("KNN")
confusion(mknn, test.clean$popular, quietly = TRUE)

```

2) Discuss which classifier you prefer and why. 

Looking at the metrics above, it is clear that QDA performs the worst for this data set. It has the lowest sensitivity and highest sensitivity. Part of this may have to do with the fact that we had to cull so many variables to meet the strict independence assumption. Ths, we had less "good" data going into our QDA model.  

The other two--LDA and KNN--are both strong classifiers. KNN has marginally better sensitivity than LDA (.9986 vs .9925). KNN's specificity is also an order of magnitude better than LDA (.003 vs .02). I would recommend using KNN for this publisher, but with a higher k (as discussed previously).



